---
title: "RAGç³»ç»Ÿä»é›¶æ­å»ºï¼šè®©AIæ‹¥æœ‰ã€Œç§äººçŸ¥è¯†åº“ã€"
date: 2025-12-18T10:00:00+08:00
draft: false
tags: ["RAG", "Retrieval Augmented Generation", "å‘é‡æ•°æ®åº“", "LangChain", "Embedding", "å®ç”¨æ•™ç¨‹"]
categories: ["å¼€å‘æŒ‡å—", "AI Agent"]
excerpt: "è®©ChatGPTè¯»æ‡‚ä½ çš„å…¬å¸æ–‡æ¡£ï¼æœ¬æ–‡æ‰‹æŠŠæ‰‹æ•™ä½ æ­å»ºRAGç³»ç»Ÿï¼Œä»æ–‡æ¡£åˆ‡å—ã€å‘é‡åµŒå…¥åˆ°è¯­ä¹‰æ£€ç´¢ï¼Œå®Œæ•´ä»£ç +é¿å‘æŒ‡å—ï¼Œæ–°æ‰‹ä¹Ÿèƒ½2å°æ—¶ä¸Šæ‰‹ã€‚"
---

## å¼€åœºï¼šè€æ¿çš„ã€Œçµé­‚æ‹·é—®ã€

**è€æ¿**ï¼š"å°ç‹ï¼Œèƒ½ä¸èƒ½è®©ChatGPTå›ç­”å…³äºæˆ‘ä»¬å…¬å¸äº§å“çš„é—®é¢˜ï¼Ÿ"

**å°ç‹**ï¼š"é¢...ChatGPTä¸çŸ¥é“æˆ‘ä»¬å…¬å¸çš„äº‹æƒ…å•Š..."

**è€æ¿**ï¼š"é‚£å°±æ•™å®ƒï¼"

**å°ç‹**ï¼š"..."

è¿™ä¸ªåœºæ™¯ä½ ç†Ÿæ‚‰å—ï¼Ÿåˆ«æ…Œï¼Œ**RAGï¼ˆRetrieval Augmented Generationï¼‰** å°±æ˜¯ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜è€Œç”Ÿçš„ã€‚

ç®€å•è¯´ï¼ŒRAG = **è®©AIè¯»å®Œä½ çš„æ–‡æ¡£åå†å›ç­”é—®é¢˜**ã€‚

---

## ç¬¬ä¸€ç« ï¼šRAGæ˜¯ä»€ä¹ˆï¼Ÿä¸€å¼ å›¾è¯´æ¸…æ¥š

![RAGæ¶æ„å›¾](/images/tutorials/rag-architecture.png)

### æ ¸å¿ƒæµç¨‹

```
ç”¨æˆ·æé—® â†’ è¯­ä¹‰æœç´¢ç›¸å…³æ–‡æ¡£ â†’ æŠŠæ–‡æ¡£+é—®é¢˜ä¸€èµ·å‘ç»™LLM â†’ ç”Ÿæˆå›ç­”
```

### ç±»æ¯”ç†è§£

æƒ³è±¡ä½ æ˜¯ä¸€ä¸ª**å¼€å·è€ƒè¯•çš„å­¦ç”Ÿ**ï¼š
- **ä¼ ç»ŸLLM**ï¼šé—­å·è€ƒè¯•ï¼Œåªèƒ½é è„‘å­é‡Œè®°ä½çš„çŸ¥è¯†
- **RAG**ï¼šå¼€å·è€ƒè¯•ï¼Œå¯ä»¥ç¿»ä¹¦æŸ¥èµ„æ–™å†å›ç­”

**RAGçš„æœ¬è´¨ï¼šç”¨æ£€ç´¢ä»£æ›¿è®°å¿†ã€‚**

---

## ç¬¬äºŒç« ï¼šåŠ¨æ‰‹æ­å»º â€”â€” å®Œæ•´ä»£ç 

### Step 1: ç¯å¢ƒå‡†å¤‡

```bash
# å®‰è£…ä¾èµ–
pip install langchain langchain-openai chromadb tiktoken
```

### Step 2: æ–‡æ¡£åŠ è½½ä¸åˆ‡å—

```python
from langchain.document_loaders import TextLoader, DirectoryLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter

# åŠ è½½æ–‡æ¡£
def load_documents(path: str):
    """åŠ è½½æŒ‡å®šç›®å½•ä¸‹çš„æ‰€æœ‰æ–‡æ¡£"""
    loader = DirectoryLoader(
        path,
        glob="**/*.txt",  # åŒ¹é…æ‰€æœ‰txtæ–‡ä»¶
        loader_cls=TextLoader
    )
    documents = loader.load()
    print(f"ğŸ“š åŠ è½½äº† {len(documents)} ä¸ªæ–‡æ¡£")
    return documents

# æ–‡æ¡£åˆ‡å—
def split_documents(documents, chunk_size=500, chunk_overlap=50):
    """å°†é•¿æ–‡æ¡£åˆ‡åˆ†æˆå°å—"""
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=chunk_size,      # æ¯å—500å­—ç¬¦
        chunk_overlap=chunk_overlap, # å—ä¹‹é—´é‡å 50å­—ç¬¦
        separators=["\n\n", "\n", "ã€‚", "ï¼", "ï¼Ÿ", "ï¼Œ", " "]
    )
    chunks = splitter.split_documents(documents)
    print(f"âœ‚ï¸ åˆ‡åˆ†æˆ {len(chunks)} ä¸ªæ–‡æœ¬å—")
    return chunks

# ä½¿ç”¨ç¤ºä¾‹
docs = load_documents("./my_documents")
chunks = split_documents(docs)
```

### åˆ‡å—ç­–ç•¥å¯¹æ¯”

| ç­–ç•¥ | é€‚ç”¨åœºæ™¯ | æ¨èå¤§å° |
|:---|:---|:---|
| **å°å—** (256-512) | ç²¾ç¡®é—®ç­”ã€FAQ | è¿½æ±‚ç²¾åº¦ |
| **ä¸­å—** (512-1000) | é€šç”¨åœºæ™¯ | å¹³è¡¡é€‰æ‹© |
| **å¤§å—** (1000-2000) | éœ€è¦ä¸Šä¸‹æ–‡ | è¿½æ±‚å®Œæ•´æ€§ |

**é»„é‡‘æ³•åˆ™**ï¼šé‡å 10-20%ï¼Œé¿å…ä¿¡æ¯è¢«åˆ‡æ–­ã€‚

### Step 3: å‘é‡åµŒå…¥ä¸å­˜å‚¨

```python
from langchain_openai import OpenAIEmbeddings
from langchain.vectorstores import Chroma

# åˆå§‹åŒ–åµŒå…¥æ¨¡å‹
embeddings = OpenAIEmbeddings(
    model="text-embedding-3-small",  # æ¨èï¼šä¾¿å®œä¸”æ•ˆæœå¥½
    # model="text-embedding-3-large",  # æ›´å¼ºä½†æ›´è´µ
)

# åˆ›å»ºå‘é‡æ•°æ®åº“
def create_vector_store(chunks, persist_directory="./chroma_db"):
    """å°†æ–‡æœ¬å—è½¬æ¢ä¸ºå‘é‡å¹¶å­˜å‚¨"""
    vectorstore = Chroma.from_documents(
        documents=chunks,
        embedding=embeddings,
        persist_directory=persist_directory
    )
    vectorstore.persist()  # æŒä¹…åŒ–åˆ°ç£ç›˜
    print(f"ğŸ’¾ å‘é‡æ•°æ®åº“å·²ä¿å­˜åˆ° {persist_directory}")
    return vectorstore

# åŠ è½½å·²æœ‰çš„å‘é‡æ•°æ®åº“
def load_vector_store(persist_directory="./chroma_db"):
    """åŠ è½½å·²å­˜åœ¨çš„å‘é‡æ•°æ®åº“"""
    vectorstore = Chroma(
        persist_directory=persist_directory,
        embedding_function=embeddings
    )
    return vectorstore

# ä½¿ç”¨
vectorstore = create_vector_store(chunks)
```

### Step 4: æ£€ç´¢ä¸ç”Ÿæˆ

```python
from langchain_openai import ChatOpenAI
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate

# åˆå§‹åŒ–LLM
llm = ChatOpenAI(
    model="gpt-4-turbo-preview",
    temperature=0.2  # é™ä½éšæœºæ€§ï¼Œå¢åŠ å‡†ç¡®æ€§
)

# è‡ªå®šä¹‰Promptæ¨¡æ¿
PROMPT_TEMPLATE = """
ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„çŸ¥è¯†åº“åŠ©æ‰‹ã€‚æ ¹æ®ä»¥ä¸‹æä¾›çš„ä¸Šä¸‹æ–‡ä¿¡æ¯å›ç­”ç”¨æˆ·é—®é¢˜ã€‚

ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼š
{context}

ç”¨æˆ·é—®é¢˜ï¼š{question}

å›ç­”è¦æ±‚ï¼š
1. åªæ ¹æ®ä¸Šä¸‹æ–‡ä¿¡æ¯å›ç­”ï¼Œä¸è¦ç¼–é€ 
2. å¦‚æœä¸Šä¸‹æ–‡ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯·æ˜ç¡®å‘ŠçŸ¥
3. å›ç­”è¦ç®€æ´ã€ä¸“ä¸š

å›ç­”ï¼š
"""

prompt = PromptTemplate(
    template=PROMPT_TEMPLATE,
    input_variables=["context", "question"]
)

# åˆ›å»ºæ£€ç´¢é“¾
def create_qa_chain(vectorstore):
    """åˆ›å»ºé—®ç­”é“¾"""
    retriever = vectorstore.as_retriever(
        search_type="similarity",
        search_kwargs={"k": 3}  # æ£€ç´¢æœ€ç›¸å…³çš„3ä¸ªæ–‡æ¡£å—
    )
    
    qa_chain = RetrievalQA.from_chain_type(
        llm=llm,
        chain_type="stuff",  # æŠŠæ‰€æœ‰æ£€ç´¢åˆ°çš„å†…å®¹å¡ç»™LLM
        retriever=retriever,
        return_source_documents=True,
        chain_type_kwargs={"prompt": prompt}
    )
    return qa_chain

# ä½¿ç”¨
qa_chain = create_qa_chain(vectorstore)
```

### Step 5: å¼€å§‹é—®ç­”ï¼

```python
def ask(question: str):
    """æé—®å¹¶è·å–å›ç­”"""
    result = qa_chain({"query": question})
    
    print("ğŸ¤– å›ç­”ï¼š")
    print(result["result"])
    
    print("\nğŸ“„ å‚è€ƒæ¥æºï¼š")
    for i, doc in enumerate(result["source_documents"], 1):
        print(f"  [{i}] {doc.metadata.get('source', 'æœªçŸ¥')} - {doc.page_content[:100]}...")
    
    return result

# ç¤ºä¾‹
ask("æˆ‘ä»¬å…¬å¸çš„é€€æ¬¾æ”¿ç­–æ˜¯ä»€ä¹ˆï¼Ÿ")
ask("äº§å“ä¿ä¿®æœŸæ˜¯å¤šä¹…ï¼Ÿ")
```

---

## ç¬¬ä¸‰ç« ï¼šè¿›é˜¶ä¼˜åŒ– â€”â€” è®©RAGæ›´èªæ˜

### ä¼˜åŒ–1ï¼šæ··åˆæ£€ç´¢ï¼ˆHybrid Searchï¼‰

è¯­ä¹‰æœç´¢æœ‰æ—¶ä¼šé”™è¿‡å…³é”®è¯åŒ¹é…ï¼Œæ··åˆæ£€ç´¢ä¸¤è€…çš„ä¼˜ç‚¹ï¼š

```python
from langchain.retrievers import BM25Retriever, EnsembleRetriever

# BM25å…³é”®è¯æ£€ç´¢
bm25_retriever = BM25Retriever.from_documents(chunks)
bm25_retriever.k = 3

# å‘é‡è¯­ä¹‰æ£€ç´¢
vector_retriever = vectorstore.as_retriever(search_kwargs={"k": 3})

# æ··åˆæ£€ç´¢å™¨
hybrid_retriever = EnsembleRetriever(
    retrievers=[bm25_retriever, vector_retriever],
    weights=[0.3, 0.7]  # è¯­ä¹‰æƒé‡é«˜ä¸€äº›
)
```

### ä¼˜åŒ–2ï¼šé‡æ’åºï¼ˆRerankingï¼‰

æ£€ç´¢åï¼Œç”¨æ›´å¼ºçš„æ¨¡å‹å¯¹ç»“æœé‡æ–°æ’åºï¼š

```python
from langchain.retrievers import ContextualCompressionRetriever
from langchain.retrievers.document_compressors import CrossEncoderReranker

# ä½¿ç”¨Cross-Encoderé‡æ’åº
reranker = CrossEncoderReranker(
    model_name="cross-encoder/ms-marco-MiniLM-L-6-v2",
    top_n=3
)

# ç»„åˆæ£€ç´¢å™¨
compression_retriever = ContextualCompressionRetriever(
    base_compressor=reranker,
    base_retriever=hybrid_retriever
)
```

### ä¼˜åŒ–3ï¼šæŸ¥è¯¢æ‰©å±•ï¼ˆQuery Expansionï¼‰

ç”¨LLMæ”¹å†™ç”¨æˆ·é—®é¢˜ï¼Œæé«˜æ£€ç´¢æ•ˆæœï¼š

```python
def expand_query(original_query: str) -> list:
    """ç”¨LLMæ‰©å±•åŸå§‹æŸ¥è¯¢"""
    expansion_prompt = f"""
    åŸå§‹é—®é¢˜ï¼š{original_query}
    
    è¯·ç”Ÿæˆ3ä¸ªè¯­ä¹‰ç›¸ä¼¼ä½†è¡¨è¾¾ä¸åŒçš„é—®é¢˜ï¼Œç”¨äºæœç´¢ç›¸å…³æ–‡æ¡£ï¼š
    1.
    2.
    3.
    """
    
    response = llm.invoke(expansion_prompt)
    expanded_queries = [original_query] + parse_queries(response)
    return expanded_queries
```

---

## ç¬¬å››ç« ï¼šé¿å‘æŒ‡å—

### å‘1ï¼šæ–‡æ¡£åˆ‡å—å¤ªå¤§æˆ–å¤ªå°

**ç—‡çŠ¶**ï¼šå›ç­”ä¸å‡†ç¡®æˆ–ç¼ºå°‘ä¸Šä¸‹æ–‡

**è§£å†³**ï¼š
- å°è¯•ä¸åŒçš„chunk_sizeï¼ˆ256/512/1024ï¼‰
- å¢åŠ chunk_overlapé˜²æ­¢ä¿¡æ¯åˆ‡æ–­
- å¯¹ä¸åŒç±»å‹æ–‡æ¡£ç”¨ä¸åŒç­–ç•¥

### å‘2ï¼šæ£€ç´¢åˆ°äº†ä½†å›ç­”é”™äº†

**ç—‡çŠ¶**ï¼šæ£€ç´¢åˆ°äº†ç›¸å…³æ–‡æ¡£ï¼Œä½†LLMç†è§£é”™è¯¯

**è§£å†³**ï¼š
- ä¼˜åŒ–Promptï¼Œæ˜ç¡®å‘Šè¯‰LLMåªç”¨ä¸Šä¸‹æ–‡å›ç­”
- å¢åŠ æ£€ç´¢æ•°é‡ï¼ˆk=5ï¼‰
- ä½¿ç”¨æ›´å¼ºçš„LLMï¼ˆGPT-4 > GPT-3.5ï¼‰

### å‘3ï¼šæ£€ç´¢ä¸åˆ°ç›¸å…³å†…å®¹

**ç—‡çŠ¶**ï¼šæ˜æ˜æœ‰ç›¸å…³æ–‡æ¡£ï¼Œä½†æ£€ç´¢ä¸åˆ°

**è§£å†³**ï¼š
- æ¢æ›´å¥½çš„Embeddingæ¨¡å‹
- ä½¿ç”¨æ··åˆæ£€ç´¢
- æ£€æŸ¥æ–‡æ¡£é¢„å¤„ç†ï¼ˆå»æ‰å™ªéŸ³ï¼‰

### å‘4ï¼šå“åº”å¤ªæ…¢

**ç—‡çŠ¶**ï¼šç­‰å¾…æ—¶é—´è¿‡é•¿

**è§£å†³**ï¼š
- å‡å°‘æ£€ç´¢æ•°é‡ï¼ˆk=3è¶³å¤Ÿï¼‰
- ä½¿ç”¨æ›´å¿«çš„Embeddingï¼ˆtext-embedding-3-smallï¼‰
- è€ƒè™‘å¼‚æ­¥å¤„ç†

---

## ç¬¬äº”ç« ï¼šç”Ÿäº§éƒ¨ç½²æ¸…å•

### âœ… ä¸Šçº¿å‰æ£€æŸ¥

| æ£€æŸ¥é¡¹ | è¯´æ˜ |
|:---|:---|
| æ–‡æ¡£æ›´æ–°æœºåˆ¶ | æ–°æ–‡æ¡£å¦‚ä½•è‡ªåŠ¨å…¥åº“ï¼Ÿ |
| å‘é‡åº“å¤‡ä»½ | å®šæœŸå¤‡ä»½ChromaDB |
| é”™è¯¯å¤„ç† | æ£€ç´¢å¤±è´¥ã€LLMè¶…æ—¶æ€ä¹ˆåŠï¼Ÿ |
| ç›‘æ§æŒ‡æ ‡ | æ£€ç´¢å‡†ç¡®ç‡ã€å“åº”æ—¶é—´ã€ç”¨æˆ·æ»¡æ„åº¦ |
| æˆæœ¬æ§åˆ¶ | Embeddingå’ŒLLMçš„APIè´¹ç”¨ |

### æ¨èæŠ€æœ¯æ ˆ

| ç»„ä»¶ | å¼€å‘ç¯å¢ƒ | ç”Ÿäº§ç¯å¢ƒ |
|:---|:---|:---|
| **å‘é‡æ•°æ®åº“** | ChromaDB | Pinecone / Weaviate / Milvus |
| **Embedding** | OpenAI | OpenAI / Cohere / è‡ªå»º |
| **LLM** | GPT-4 | GPT-4 / Claude / è‡ªå»º |
| **æ¡†æ¶** | LangChain | LangChain / LlamaIndex |

---

## å®Œæ•´ä»£ç æ±‡æ€»

```python
"""
RAGç³»ç»Ÿå®Œæ•´å®ç°
è¿è¡Œï¼špython rag_demo.py
"""

import os
from langchain.document_loaders import DirectoryLoader, TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain.vectorstores import Chroma
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate

# é…ç½®
os.environ["OPENAI_API_KEY"] = "your-api-key"
DOCS_PATH = "./documents"
DB_PATH = "./chroma_db"

def main():
    # 1. åŠ è½½æ–‡æ¡£
    loader = DirectoryLoader(DOCS_PATH, glob="**/*.txt", loader_cls=TextLoader)
    docs = loader.load()
    print(f"ğŸ“š åŠ è½½ {len(docs)} ä¸ªæ–‡æ¡£")
    
    # 2. åˆ‡å—
    splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
    chunks = splitter.split_documents(docs)
    print(f"âœ‚ï¸ åˆ‡åˆ† {len(chunks)} ä¸ªæ–‡æœ¬å—")
    
    # 3. å‘é‡åŒ–å­˜å‚¨
    embeddings = OpenAIEmbeddings(model="text-embedding-3-small")
    vectorstore = Chroma.from_documents(chunks, embeddings, persist_directory=DB_PATH)
    vectorstore.persist()
    print("ğŸ’¾ å‘é‡æ•°æ®åº“å·²ä¿å­˜")
    
    # 4. åˆ›å»ºé—®ç­”é“¾
    llm = ChatOpenAI(model="gpt-4-turbo-preview", temperature=0.2)
    
    prompt = PromptTemplate(
        template="æ ¹æ®ä»¥ä¸‹ä¸Šä¸‹æ–‡å›ç­”é—®é¢˜ã€‚\nä¸Šä¸‹æ–‡ï¼š{context}\né—®é¢˜ï¼š{question}\nå›ç­”ï¼š",
        input_variables=["context", "question"]
    )
    
    qa_chain = RetrievalQA.from_chain_type(
        llm=llm,
        retriever=vectorstore.as_retriever(search_kwargs={"k": 3}),
        return_source_documents=True,
        chain_type_kwargs={"prompt": prompt}
    )
    
    # 5. äº¤äº’å¼é—®ç­”
    print("\nğŸ¤– RAGç³»ç»Ÿå°±ç»ªï¼è¾“å…¥é—®é¢˜å¼€å§‹å¯¹è¯ï¼ˆè¾“å…¥qé€€å‡ºï¼‰\n")
    while True:
        question = input("ä½ : ")
        if question.lower() == 'q':
            break
        result = qa_chain({"query": question})
        print(f"AI: {result['result']}\n")

if __name__ == "__main__":
    main()
```

---

## ç»“è¯­

RAGæ˜¯è®©AI"ç§äººå®šåˆ¶"çš„æœ€ä½³æ–¹æ¡ˆã€‚ä¸éœ€è¦æ˜‚è´µçš„å¾®è°ƒï¼Œä¸éœ€è¦é‡æ–°è®­ç»ƒæ¨¡å‹ï¼Œåªéœ€è¦æŠŠä½ çš„æ–‡æ¡£å–‚ç»™å®ƒã€‚

**3ä¸ªè¦ç‚¹å¸¦å›å»**ï¼š
1. **åˆ‡å—æ˜¯å…³é”®**ï¼š500å­—ç¬¦+50é‡å æ˜¯å¥½çš„èµ·ç‚¹
2. **æ··åˆæ£€ç´¢æ›´å¼º**ï¼šè¯­ä¹‰+å…³é”®è¯åŒä¿é™©
3. **Promptå¾ˆé‡è¦**ï¼šæ˜ç¡®å‘Šè¯‰LLM"åªç”¨ä¸Šä¸‹æ–‡å›ç­”"

ç°åœ¨ï¼Œå»è®©ä½ çš„ChatGPTå˜æˆ"å…¬å¸ç™¾ç§‘å…¨ä¹¦"å§ï¼ğŸš€

