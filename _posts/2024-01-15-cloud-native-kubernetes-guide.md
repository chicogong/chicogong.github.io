---
layout: post
title: "äº‘åŸç”ŸKuberneteså®æˆ˜æŒ‡å—ï¼šä»å…¥é—¨åˆ°ç”Ÿäº§ç¯å¢ƒéƒ¨ç½²"
date: 2024-01-15 11:45:00 +0800
categories: [äº‘åŸç”Ÿ, DevOps]
tags: [Kubernetes, Docker, å®¹å™¨ç¼–æ’, å¾®æœåŠ¡, DevOps]
---

Kuberneteså·²ç»æˆä¸ºç°ä»£äº‘åŸç”Ÿåº”ç”¨éƒ¨ç½²çš„æ ‡å‡†å¹³å°ã€‚æœ¬æ–‡å°†ä»åŸºç¡€æ¦‚å¿µå¼€å§‹ï¼Œé€æ­¥æ·±å…¥åˆ°ç”Ÿäº§ç¯å¢ƒçš„æœ€ä½³å®è·µï¼Œå¸®åŠ©ä½ å…¨é¢æŒæ¡Kubernetesçš„æ ¸å¿ƒæŠ€èƒ½ã€‚

## ä»€ä¹ˆæ˜¯Kubernetesï¼Ÿ

Kubernetesï¼ˆç®€ç§°K8sï¼‰æ˜¯ä¸€ä¸ªå¼€æºçš„å®¹å™¨ç¼–æ’ç³»ç»Ÿï¼Œç”¨äºè‡ªåŠ¨åŒ–åº”ç”¨ç¨‹åºçš„éƒ¨ç½²ã€æ‰©å±•å’Œç®¡ç†ã€‚å®ƒæä¾›äº†ï¼š

- **å®¹å™¨ç¼–æ’**ï¼šç®¡ç†å¤§é‡å®¹å™¨çš„ç”Ÿå‘½å‘¨æœŸ
- **æœåŠ¡å‘ç°**ï¼šè‡ªåŠ¨å‘ç°å’Œè¿æ¥æœåŠ¡
- **è´Ÿè½½å‡è¡¡**ï¼šåˆ†å‘æµé‡åˆ°å¥åº·çš„å®ä¾‹
- **è‡ªåŠ¨æ‰©ç¼©å®¹**ï¼šæ ¹æ®è´Ÿè½½è‡ªåŠ¨è°ƒæ•´å®ä¾‹æ•°é‡
- **æ»šåŠ¨æ›´æ–°**ï¼šé›¶åœæœºæ—¶é—´çš„åº”ç”¨æ›´æ–°

## æ ¸å¿ƒæ¦‚å¿µè¯¦è§£

### 1. åŸºç¡€å¯¹è±¡

```yaml
# pod.yaml - Podæ˜¯K8sä¸­æœ€å°çš„éƒ¨ç½²å•å…ƒ
apiVersion: v1
kind: Pod
metadata:
  name: my-app-pod
  labels:
    app: my-app
    version: v1.0
spec:
  containers:
  - name: app-container
    image: nginx:1.20
    ports:
    - containerPort: 80
    resources:
      requests:
        memory: "64Mi"
        cpu: "250m"
      limits:
        memory: "128Mi"
        cpu: "500m"
    env:
    - name: ENV_VAR
      value: "production"
    livenessProbe:
      httpGet:
        path: /health
        port: 80
      initialDelaySeconds: 30
      periodSeconds: 10
    readinessProbe:
      httpGet:
        path: /ready
        port: 80
      initialDelaySeconds: 5
      periodSeconds: 5

---
# service.yaml - Serviceæä¾›ç¨³å®šçš„ç½‘ç»œç«¯ç‚¹
apiVersion: v1
kind: Service
metadata:
  name: my-app-service
spec:
  selector:
    app: my-app
  ports:
  - port: 80
    targetPort: 80
    protocol: TCP
  type: ClusterIP

---
# deployment.yaml - Deploymentç®¡ç†Podçš„å‰¯æœ¬
apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-app-deployment
  labels:
    app: my-app
spec:
  replicas: 3
  selector:
    matchLabels:
      app: my-app
  template:
    metadata:
      labels:
        app: my-app
    spec:
      containers:
      - name: app-container
        image: nginx:1.20
        ports:
        - containerPort: 80
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 1
```

### 2. é…ç½®ç®¡ç†

```yaml
# configmap.yaml - é…ç½®æ•°æ®
apiVersion: v1
kind: ConfigMap
metadata:
  name: app-config
data:
  database_url: "postgresql://localhost:5432/mydb"
  debug_mode: "false"
  app.properties: |
    server.port=8080
    logging.level=INFO
    cache.enabled=true

---
# secret.yaml - æ•æ„Ÿä¿¡æ¯
apiVersion: v1
kind: Secret
metadata:
  name: app-secrets
type: Opaque
data:
  # æ³¨æ„ï¼šè¿™äº›å€¼éœ€è¦base64ç¼–ç 
  database-password: cGFzc3dvcmQxMjM=  # password123
  api-key: YWJjZGVmZ2hpams=            # abcdefghijk

---
# deployment-with-config.yaml - ä½¿ç”¨é…ç½®çš„éƒ¨ç½²
apiVersion: apps/v1
kind: Deployment
metadata:
  name: app-with-config
spec:
  replicas: 2
  selector:
    matchLabels:
      app: configured-app
  template:
    metadata:
      labels:
        app: configured-app
    spec:
      containers:
      - name: app
        image: my-app:v1.0
        env:
        # ä»ConfigMapè·å–ç¯å¢ƒå˜é‡
        - name: DATABASE_URL
          valueFrom:
            configMapKeyRef:
              name: app-config
              key: database_url
        # ä»Secretè·å–æ•æ„Ÿä¿¡æ¯
        - name: DB_PASSWORD
          valueFrom:
            secretKeyRef:
              name: app-secrets
              key: database-password
        volumeMounts:
        # æŒ‚è½½é…ç½®æ–‡ä»¶
        - name: config-volume
          mountPath: /etc/config
        - name: secret-volume
          mountPath: /etc/secrets
          readOnly: true
      volumes:
      - name: config-volume
        configMap:
          name: app-config
      - name: secret-volume
        secret:
          secretName: app-secrets
```

### 3. æŒä¹…åŒ–å­˜å‚¨

```yaml
# pv.yaml - æŒä¹…å·
apiVersion: v1
kind: PersistentVolume
metadata:
  name: mysql-pv
spec:
  capacity:
    storage: 10Gi
  accessModes:
    - ReadWriteOnce
  persistentVolumeReclaimPolicy: Retain
  storageClassName: standard
  hostPath:
    path: /data/mysql

---
# pvc.yaml - æŒä¹…å·å£°æ˜
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: mysql-pvc
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 10Gi
  storageClassName: standard

---
# mysql-deployment.yaml - ä½¿ç”¨æŒä¹…å­˜å‚¨çš„æ•°æ®åº“
apiVersion: apps/v1
kind: Deployment
metadata:
  name: mysql
spec:
  replicas: 1
  selector:
    matchLabels:
      app: mysql
  template:
    metadata:
      labels:
        app: mysql
    spec:
      containers:
      - name: mysql
        image: mysql:8.0
        env:
        - name: MYSQL_ROOT_PASSWORD
          valueFrom:
            secretKeyRef:
              name: mysql-secret
              key: root-password
        - name: MYSQL_DATABASE
          value: "myapp"
        ports:
        - containerPort: 3306
        volumeMounts:
        - name: mysql-storage
          mountPath: /var/lib/mysql
        resources:
          requests:
            memory: "512Mi"
            cpu: "500m"
          limits:
            memory: "1Gi"
            cpu: "1000m"
      volumes:
      - name: mysql-storage
        persistentVolumeClaim:
          claimName: mysql-pvc
```

## é«˜çº§åŠŸèƒ½å®æˆ˜

### 1. è‡ªåŠ¨æ‰©ç¼©å®¹ï¼ˆHPAï¼‰

```yaml
# hpa.yaml - æ°´å¹³Podè‡ªåŠ¨æ‰©ç¼©å®¹
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: my-app-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: my-app-deployment
  minReplicas: 2
  maxReplicas: 10
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 80
  behavior:
    scaleDown:
      stabilizationWindowSeconds: 300
      policies:
      - type: Percent
        value: 50
        periodSeconds: 15
    scaleUp:
      stabilizationWindowSeconds: 0
      policies:
      - type: Percent
        value: 100
        periodSeconds: 15

---
# vpa.yaml - å‚ç›´Podè‡ªåŠ¨æ‰©ç¼©å®¹
apiVersion: autoscaling.k8s.io/v1
kind: VerticalPodAutoscaler
metadata:
  name: my-app-vpa
spec:
  targetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: my-app-deployment
  updatePolicy:
    updateMode: "Auto"  # æˆ– "Off", "Initial"
  resourcePolicy:
    containerPolicies:
    - containerName: app-container
      maxAllowed:
        cpu: 2
        memory: 4Gi
      minAllowed:
        cpu: 100m
        memory: 128Mi
```

### 2. ç½‘ç»œç­–ç•¥å’Œå®‰å…¨

```yaml
# network-policy.yaml - ç½‘ç»œè®¿é—®æ§åˆ¶
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: app-network-policy
spec:
  podSelector:
    matchLabels:
      app: my-app
  policyTypes:
  - Ingress
  - Egress
  ingress:
  - from:
    - podSelector:
        matchLabels:
          role: frontend
    - namespaceSelector:
        matchLabels:
          name: trusted
    ports:
    - protocol: TCP
      port: 80
  egress:
  - to:
    - podSelector:
        matchLabels:
          app: database
    ports:
    - protocol: TCP
      port: 5432

---
# rbac.yaml - åŸºäºè§’è‰²çš„è®¿é—®æ§åˆ¶
apiVersion: v1
kind: ServiceAccount
metadata:
  name: app-service-account
  namespace: default

---
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: app-role
  namespace: default
rules:
- apiGroups: [""]
  resources: ["configmaps", "secrets"]
  verbs: ["get", "list"]
- apiGroups: ["apps"]
  resources: ["deployments"]
  verbs: ["get", "list", "patch"]

---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: app-role-binding
  namespace: default
subjects:
- kind: ServiceAccount
  name: app-service-account
  namespace: default
roleRef:
  kind: Role
  name: app-role
  apiGroup: rbac.authorization.k8s.io

---
# pod-security-policy.yaml - Podå®‰å…¨ç­–ç•¥
apiVersion: policy/v1beta1
kind: PodSecurityPolicy
metadata:
  name: restricted-psp
spec:
  privileged: false
  allowPrivilegeEscalation: false
  requiredDropCapabilities:
    - ALL
  volumes:
    - 'configMap'
    - 'emptyDir'
    - 'projected'
    - 'secret'
    - 'downwardAPI'
    - 'persistentVolumeClaim'
  runAsUser:
    rule: 'MustRunAsNonRoot'
  seLinux:
    rule: 'RunAsAny'
  fsGroup:
    rule: 'RunAsAny'
```

### 3. Ingresså’Œè´Ÿè½½å‡è¡¡

```yaml
# ingress.yaml - Ingressèµ„æº
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: app-ingress
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /
    nginx.ingress.kubernetes.io/ssl-redirect: "true"
    nginx.ingress.kubernetes.io/use-regex: "true"
    cert-manager.io/cluster-issuer: "letsencrypt-prod"
spec:
  tls:
  - hosts:
    - myapp.example.com
    secretName: app-tls-secret
  rules:
  - host: myapp.example.com
    http:
      paths:
      - path: /api/v1
        pathType: Prefix
        backend:
          service:
            name: api-service
            port:
              number: 80
      - path: /
        pathType: Prefix
        backend:
          service:
            name: frontend-service
            port:
              number: 80

---
# load-balancer-service.yaml - LoadBalancerç±»å‹æœåŠ¡
apiVersion: v1
kind: Service
metadata:
  name: app-loadbalancer
  annotations:
    service.beta.kubernetes.io/aws-load-balancer-type: nlb
    service.beta.kubernetes.io/aws-load-balancer-cross-zone-load-balancing-enabled: "true"
spec:
  type: LoadBalancer
  selector:
    app: my-app
  ports:
  - port: 80
    targetPort: 8080
    protocol: TCP
```

## CI/CDé›†æˆ

### 1. GitLab CIé…ç½®

```yaml
# .gitlab-ci.yml
stages:
  - build
  - test
  - deploy-staging
  - deploy-production

variables:
  DOCKER_REGISTRY: registry.gitlab.com/myproject
  KUBE_NAMESPACE_STAGING: myapp-staging
  KUBE_NAMESPACE_PRODUCTION: myapp-production

build:
  stage: build
  image: docker:latest
  services:
    - docker:dind
  script:
    - docker build -t $DOCKER_REGISTRY/myapp:$CI_COMMIT_SHA .
    - docker push $DOCKER_REGISTRY/myapp:$CI_COMMIT_SHA
  only:
    - main
    - develop

test:
  stage: test
  image: $DOCKER_REGISTRY/myapp:$CI_COMMIT_SHA
  script:
    - npm test
    - npm run lint
  coverage: '/Coverage: \d+\.\d+%/'

deploy-staging:
  stage: deploy-staging
  image: bitnami/kubectl:latest
  script:
    - kubectl config use-context $KUBE_CONTEXT_STAGING
    - kubectl set image deployment/myapp-deployment app=$DOCKER_REGISTRY/myapp:$CI_COMMIT_SHA -n $KUBE_NAMESPACE_STAGING
    - kubectl rollout status deployment/myapp-deployment -n $KUBE_NAMESPACE_STAGING
  environment:
    name: staging
    url: https://staging.myapp.com
  only:
    - develop

deploy-production:
  stage: deploy-production
  image: bitnami/kubectl:latest
  script:
    - kubectl config use-context $KUBE_CONTEXT_PRODUCTION
    - kubectl set image deployment/myapp-deployment app=$DOCKER_REGISTRY/myapp:$CI_COMMIT_SHA -n $KUBE_NAMESPACE_PRODUCTION
    - kubectl rollout status deployment/myapp-deployment -n $KUBE_NAMESPACE_PRODUCTION
  environment:
    name: production
    url: https://myapp.com
  when: manual
  only:
    - main
```

### 2. Helm Charts

```yaml
# Chart.yaml
apiVersion: v2
name: myapp
description: My Application Helm Chart
version: 0.1.0
appVersion: 1.0.0

# values.yaml
replicaCount: 3

image:
  repository: myapp
  tag: latest
  pullPolicy: IfNotPresent

service:
  type: ClusterIP
  port: 80
  targetPort: 8080

ingress:
  enabled: true
  annotations:
    kubernetes.io/ingress.class: nginx
  hosts:
    - host: myapp.local
      paths:
        - path: /
          pathType: Prefix
  tls: []

resources:
  limits:
    cpu: 500m
    memory: 512Mi
  requests:
    cpu: 250m
    memory: 256Mi

autoscaling:
  enabled: true
  minReplicas: 2
  maxReplicas: 10
  targetCPU: 70
  targetMemory: 80

monitoring:
  enabled: true
  serviceMonitor:
    enabled: true

# templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: {{ include "myapp.fullname" . }}
  labels:
    {{- include "myapp.labels" . | nindent 4 }}
spec:
  {{- if not .Values.autoscaling.enabled }}
  replicas: {{ .Values.replicaCount }}
  {{- end }}
  selector:
    matchLabels:
      {{- include "myapp.selectorLabels" . | nindent 6 }}
  template:
    metadata:
      labels:
        {{- include "myapp.selectorLabels" . | nindent 8 }}
    spec:
      containers:
        - name: {{ .Chart.Name }}
          image: "{{ .Values.image.repository }}:{{ .Values.image.tag | default .Chart.AppVersion }}"
          imagePullPolicy: {{ .Values.image.pullPolicy }}
          ports:
            - name: http
              containerPort: {{ .Values.service.targetPort }}
              protocol: TCP
          livenessProbe:
            httpGet:
              path: /health
              port: http
            initialDelaySeconds: 30
            periodSeconds: 10
          readinessProbe:
            httpGet:
              path: /ready
              port: http
            initialDelaySeconds: 5
            periodSeconds: 5
          resources:
            {{- toYaml .Values.resources | nindent 12 }}

# éƒ¨ç½²å‘½ä»¤
# helm install myapp ./myapp-chart
# helm upgrade myapp ./myapp-chart --set image.tag=v2.0.0
```

## ç›‘æ§å’Œæ—¥å¿—

### 1. Prometheusç›‘æ§

```yaml
# monitoring/prometheus-config.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: prometheus-config
data:
  prometheus.yml: |
    global:
      scrape_interval: 15s
      evaluation_interval: 15s
    
    rule_files:
      - "alert_rules.yml"
    
    alerting:
      alertmanagers:
        - static_configs:
            - targets:
              - alertmanager:9093
    
    scrape_configs:
      - job_name: 'kubernetes-pods'
        kubernetes_sd_configs:
          - role: pod
        relabel_configs:
          - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape]
            action: keep
            regex: true
          - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path]
            action: replace
            target_label: __metrics_path__
            regex: (.+)
      
      - job_name: 'kubernetes-services'
        kubernetes_sd_configs:
          - role: service
        relabel_configs:
          - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scrape]
            action: keep
            regex: true

---
# monitoring/grafana-dashboard.json
{
  "dashboard": {
    "id": null,
    "title": "Kubernetes Application Monitoring",
    "panels": [
      {
        "title": "CPU Usage",
        "type": "graph",
        "targets": [
          {
            "expr": "rate(container_cpu_usage_seconds_total{namespace=\"default\"}[5m])",
            "legendFormat": "{{pod}}"
          }
        ]
      },
      {
        "title": "Memory Usage",
        "type": "graph",
        "targets": [
          {
            "expr": "container_memory_working_set_bytes{namespace=\"default\"}",
            "legendFormat": "{{pod}}"
          }
        ]
      },
      {
        "title": "Request Rate",
        "type": "graph",
        "targets": [
          {
            "expr": "rate(http_requests_total[5m])",
            "legendFormat": "{{method}} {{status}}"
          }
        ]
      }
    ]
  }
}
```

### 2. æ—¥å¿—æ”¶é›†

```yaml
# logging/fluentd-daemonset.yaml
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: fluentd
  namespace: kube-system
spec:
  selector:
    matchLabels:
      name: fluentd
  template:
    metadata:
      labels:
        name: fluentd
    spec:
      containers:
      - name: fluentd
        image: fluent/fluentd-kubernetes-daemonset:v1-debian-elasticsearch
        env:
        - name: FLUENT_ELASTICSEARCH_HOST
          value: "elasticsearch.logging.svc.cluster.local"
        - name: FLUENT_ELASTICSEARCH_PORT
          value: "9200"
        volumeMounts:
        - name: varlog
          mountPath: /var/log
        - name: varlibdockercontainers
          mountPath: /var/lib/docker/containers
          readOnly: true
        - name: config-volume
          mountPath: /fluentd/etc
      volumes:
      - name: varlog
        hostPath:
          path: /var/log
      - name: varlibdockercontainers
        hostPath:
          path: /var/lib/docker/containers
      - name: config-volume
        configMap:
          name: fluentd-config

---
# logging/fluentd-config.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: fluentd-config
  namespace: kube-system
data:
  fluent.conf: |
    <source>
      @type tail
      path /var/log/containers/*.log
      pos_file /var/log/fluentd-containers.log.pos
      tag kubernetes.*
      read_from_head true
      <parse>
        @type json
        time_format %Y-%m-%dT%H:%M:%S.%NZ
      </parse>
    </source>
    
    <filter kubernetes.**>
      @type kubernetes_metadata
    </filter>
    
    <match kubernetes.**>
      @type elasticsearch
      host elasticsearch.logging.svc.cluster.local
      port 9200
      index_name kubernetes
      type_name fluentd
    </match>
```

## ç”Ÿäº§ç¯å¢ƒæœ€ä½³å®è·µ

### 1. é›†ç¾¤å®‰å…¨åŠ å›º

```bash
#!/bin/bash
# security-hardening.sh

# 1. å¯ç”¨RBAC
kubectl apply -f - <<EOF
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: pod-reader
rules:
- apiGroups: [""]
  resources: ["pods"]
  verbs: ["get", "watch", "list"]
EOF

# 2. ç½‘ç»œç­–ç•¥
kubectl apply -f - <<EOF
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: default-deny-all
spec:
  podSelector: {}
  policyTypes:
  - Ingress
  - Egress
EOF

# 3. Podå®‰å…¨æ ‡å‡†
kubectl label namespace default pod-security.kubernetes.io/enforce=restricted
kubectl label namespace default pod-security.kubernetes.io/audit=restricted
kubectl label namespace default pod-security.kubernetes.io/warn=restricted

# 4. å¯ç”¨å®¡è®¡æ—¥å¿—
cat << EOF > /etc/kubernetes/audit-policy.yaml
apiVersion: audit.k8s.io/v1
kind: Policy
rules:
- level: Metadata
  resources:
  - group: ""
    resources: ["secrets", "configmaps"]
- level: RequestResponse
  resources:
  - group: ""
    resources: [""]
  - group: "apps"
    resources: ["deployments"]
EOF

# 5. å®šæœŸæ›´æ–°å’Œè¡¥ä¸
kubectl get nodes -o wide
kubectl get pods --all-namespaces -o wide | grep -v Running
```

### 2. å¤‡ä»½å’Œç¾éš¾æ¢å¤

```yaml
# backup/velero-backup.yaml
apiVersion: velero.io/v1
kind: Backup
metadata:
  name: daily-backup
spec:
  includedNamespaces:
  - default
  - production
  excludedResources:
  - events
  - events.events.k8s.io
  storageLocation: default
  ttl: 720h0m0s  # 30 days

---
# backup/schedule-backup.yaml
apiVersion: velero.io/v1
kind: Schedule
metadata:
  name: daily-backup-schedule
spec:
  schedule: "0 2 * * *"  # æ¯å¤©å‡Œæ™¨2ç‚¹
  template:
    includedNamespaces:
    - default
    - production
    storageLocation: default
    ttl: 720h0m0s

# æ¢å¤å‘½ä»¤ç¤ºä¾‹
# velero restore create --from-backup daily-backup-20240115
```

### 3. æ€§èƒ½ä¼˜åŒ–è„šæœ¬

```python
#!/usr/bin/env python3
# k8s-optimizer.py

import subprocess
import json
import yaml
from datetime import datetime, timedelta

class K8sOptimizer:
    def __init__(self):
        self.kubectl = "kubectl"
    
    def get_resource_usage(self, namespace="default"):
        """è·å–èµ„æºä½¿ç”¨ç»Ÿè®¡"""
        cmd = f"{self.kubectl} top pods -n {namespace} --no-headers"
        result = subprocess.run(cmd.split(), capture_output=True, text=True)
        
        pods_usage = []
        for line in result.stdout.strip().split('\n'):
            if line:
                parts = line.split()
                pods_usage.append({
                    'name': parts[0],
                    'cpu': parts[1],
                    'memory': parts[2]
                })
        
        return pods_usage
    
    def find_unused_resources(self):
        """æŸ¥æ‰¾æœªä½¿ç”¨çš„èµ„æº"""
        # æŸ¥æ‰¾æœªä½¿ç”¨çš„ConfigMaps
        cmd = f"{self.kubectl} get configmaps -o json"
        result = subprocess.run(cmd.split(), capture_output=True, text=True)
        configmaps = json.loads(result.stdout)
        
        unused_configmaps = []
        for cm in configmaps['items']:
            # æ£€æŸ¥æ˜¯å¦è¢«Podä½¿ç”¨
            cmd = f"{self.kubectl} get pods -o json"
            pods_result = subprocess.run(cmd.split(), capture_output=True, text=True)
            pods = json.loads(pods_result.stdout)
            
            is_used = False
            for pod in pods['items']:
                # æ£€æŸ¥ç¯å¢ƒå˜é‡å’ŒæŒ‚è½½å·
                spec = pod.get('spec', {})
                containers = spec.get('containers', [])
                
                for container in containers:
                    # æ£€æŸ¥ç¯å¢ƒå˜é‡
                    env = container.get('env', [])
                    for env_var in env:
                        if env_var.get('valueFrom', {}).get('configMapKeyRef', {}).get('name') == cm['metadata']['name']:
                            is_used = True
                            break
                    
                    # æ£€æŸ¥æŒ‚è½½å·
                    volume_mounts = container.get('volumeMounts', [])
                    volumes = spec.get('volumes', [])
                    for volume in volumes:
                        if volume.get('configMap', {}).get('name') == cm['metadata']['name']:
                            is_used = True
                            break
            
            if not is_used:
                unused_configmaps.append(cm['metadata']['name'])
        
        return {
            'unused_configmaps': unused_configmaps
        }
    
    def optimize_resources(self):
        """èµ„æºä¼˜åŒ–å»ºè®®"""
        usage = self.get_resource_usage()
        recommendations = []
        
        for pod in usage:
            cpu_val = float(pod['cpu'].replace('m', '')) if 'm' in pod['cpu'] else float(pod['cpu']) * 1000
            memory_val = float(pod['memory'].replace('Mi', ''))
            
            # CPUä½¿ç”¨ç‡è¿‡ä½
            if cpu_val < 50:  # 50m
                recommendations.append({
                    'pod': pod['name'],
                    'type': 'CPU_UNDERUTILIZED',
                    'current': pod['cpu'],
                    'suggestion': 'Consider reducing CPU request/limit'
                })
            
            # å†…å­˜ä½¿ç”¨ç‡æ£€æŸ¥
            if memory_val < 64:  # 64Mi
                recommendations.append({
                    'pod': pod['name'],
                    'type': 'MEMORY_UNDERUTILIZED',
                    'current': pod['memory'],
                    'suggestion': 'Consider reducing memory request/limit'
                })
        
        return recommendations
    
    def generate_report(self):
        """ç”Ÿæˆä¼˜åŒ–æŠ¥å‘Š"""
        print("=" * 50)
        print("Kubernetes é›†ç¾¤ä¼˜åŒ–æŠ¥å‘Š")
        print("=" * 50)
        print(f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print()
        
        # èµ„æºä½¿ç”¨æƒ…å†µ
        usage = self.get_resource_usage()
        print("èµ„æºä½¿ç”¨æƒ…å†µ:")
        print(f"{'Podåç§°':<30} {'CPU':<10} {'å†…å­˜':<10}")
        print("-" * 50)
        for pod in usage:
            print(f"{pod['name']:<30} {pod['cpu']:<10} {pod['memory']:<10}")
        print()
        
        # ä¼˜åŒ–å»ºè®®
        recommendations = self.optimize_resources()
        if recommendations:
            print("ä¼˜åŒ–å»ºè®®:")
            for rec in recommendations:
                print(f"- {rec['pod']}: {rec['suggestion']} (å½“å‰: {rec['current']})")
        else:
            print("æš‚æ— ä¼˜åŒ–å»ºè®®")
        print()
        
        # æœªä½¿ç”¨èµ„æº
        unused = self.find_unused_resources()
        if unused['unused_configmaps']:
            print("æœªä½¿ç”¨çš„ConfigMaps:")
            for cm in unused['unused_configmaps']:
                print(f"- {cm}")
        else:
            print("æœªå‘ç°æœªä½¿ç”¨çš„ConfigMaps")

if __name__ == "__main__":
    optimizer = K8sOptimizer()
    optimizer.generate_report()
```

## æ•…éšœæ’æŸ¥æŒ‡å—

### 1. å¸¸ç”¨è°ƒè¯•å‘½ä»¤

```bash
#!/bin/bash
# k8s-debug.sh

# é›†ç¾¤çŠ¶æ€æ£€æŸ¥
echo "=== é›†ç¾¤çŠ¶æ€ ==="
kubectl cluster-info
kubectl get nodes -o wide
kubectl get pods --all-namespaces | grep -v Running

# èµ„æºä½¿ç”¨æƒ…å†µ
echo "=== èµ„æºä½¿ç”¨ ==="
kubectl top nodes
kubectl top pods --all-namespaces --sort-by=cpu
kubectl top pods --all-namespaces --sort-by=memory

# äº‹ä»¶æŸ¥çœ‹
echo "=== æœ€è¿‘äº‹ä»¶ ==="
kubectl get events --sort-by=.metadata.creationTimestamp

# ç½‘ç»œè¿æ¥æµ‹è¯•
echo "=== ç½‘ç»œæµ‹è¯• ==="
kubectl run tmp-shell --rm -i --tty --image nicolaka/netshoot -- /bin/bash

# æ—¥å¿—æŸ¥çœ‹
echo "=== Podæ—¥å¿— ==="
kubectl logs -f deployment/my-app --tail=100

# è¿›å…¥Podè°ƒè¯•
echo "=== Podè°ƒè¯• ==="
kubectl exec -it pod-name -- /bin/bash

# ç«¯å£è½¬å‘
echo "=== ç«¯å£è½¬å‘ ==="
kubectl port-forward service/my-service 8080:80
```

### 2. é—®é¢˜è¯Šæ–­è„šæœ¬

```python
#!/usr/bin/env python3
# k8s-troubleshoot.py

import subprocess
import json
import re
from datetime import datetime

class K8sTroubleshooter:
    def __init__(self):
        self.kubectl = "kubectl"
        self.issues = []
    
    def check_pod_status(self):
        """æ£€æŸ¥PodçŠ¶æ€"""
        cmd = f"{self.kubectl} get pods --all-namespaces -o json"
        result = subprocess.run(cmd.split(), capture_output=True, text=True)
        pods = json.loads(result.stdout)
        
        for pod in pods['items']:
            pod_name = pod['metadata']['name']
            namespace = pod['metadata']['namespace']
            status = pod['status']['phase']
            
            if status != 'Running':
                # è·å–è¯¦ç»†çŠ¶æ€
                conditions = pod['status'].get('conditions', [])
                container_statuses = pod['status'].get('containerStatuses', [])
                
                issue = {
                    'type': 'POD_NOT_RUNNING',
                    'pod': pod_name,
                    'namespace': namespace,
                    'status': status,
                    'conditions': conditions,
                    'containers': container_statuses
                }
                self.issues.append(issue)
    
    def check_resource_limits(self):
        """æ£€æŸ¥èµ„æºé™åˆ¶"""
        cmd = f"{self.kubectl} describe nodes"
        result = subprocess.run(cmd.split(), capture_output=True, text=True)
        
        # è§£æèŠ‚ç‚¹èµ„æºä½¿ç”¨æƒ…å†µ
        lines = result.stdout.split('\n')
        for i, line in enumerate(lines):
            if 'Allocated resources:' in line:
                # æŸ¥æ‰¾CPUå’Œå†…å­˜ä½¿ç”¨ç‡
                for j in range(i+1, min(i+10, len(lines))):
                    if 'cpu' in lines[j]:
                        cpu_match = re.search(r'(\d+)%', lines[j])
                        if cpu_match and int(cpu_match.group(1)) > 80:
                            self.issues.append({
                                'type': 'HIGH_CPU_USAGE',
                                'usage': cpu_match.group(1) + '%'
                            })
                    elif 'memory' in lines[j]:
                        memory_match = re.search(r'(\d+)%', lines[j])
                        if memory_match and int(memory_match.group(1)) > 80:
                            self.issues.append({
                                'type': 'HIGH_MEMORY_USAGE',
                                'usage': memory_match.group(1) + '%'
                            })
    
    def check_storage_issues(self):
        """æ£€æŸ¥å­˜å‚¨é—®é¢˜"""
        cmd = f"{self.kubectl} get pvc --all-namespaces -o json"
        result = subprocess.run(cmd.split(), capture_output=True, text=True)
        pvcs = json.loads(result.stdout)
        
        for pvc in pvcs['items']:
            status = pvc['status'].get('phase')
            if status != 'Bound':
                self.issues.append({
                    'type': 'PVC_NOT_BOUND',
                    'pvc': pvc['metadata']['name'],
                    'namespace': pvc['metadata']['namespace'],
                    'status': status
                })
    
    def check_network_policies(self):
        """æ£€æŸ¥ç½‘ç»œç­–ç•¥"""
        cmd = f"{self.kubectl} get networkpolicies --all-namespaces -o json"
        result = subprocess.run(cmd.split(), capture_output=True, text=True)
        
        if result.returncode == 0:
            policies = json.loads(result.stdout)
            if not policies['items']:
                self.issues.append({
                    'type': 'NO_NETWORK_POLICIES',
                    'message': 'æœªå‘ç°ç½‘ç»œç­–ç•¥ï¼Œå¯èƒ½å­˜åœ¨å®‰å…¨é£é™©'
                })
    
    def generate_report(self):
        """ç”Ÿæˆæ•…éšœæ’æŸ¥æŠ¥å‘Š"""
        print("=" * 60)
        print("Kubernetes æ•…éšœæ’æŸ¥æŠ¥å‘Š")
        print("=" * 60)
        print(f"æ£€æŸ¥æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print()
        
        if not self.issues:
            print("âœ… æœªå‘ç°æ˜æ˜¾é—®é¢˜")
            return
        
        print(f"ğŸ” å‘ç° {len(self.issues)} ä¸ªé—®é¢˜:")
        print()
        
        for i, issue in enumerate(self.issues, 1):
            print(f"{i}. {self.format_issue(issue)}")
            print()
    
    def format_issue(self, issue):
        """æ ¼å¼åŒ–é—®é¢˜æè¿°"""
        issue_type = issue['type']
        
        if issue_type == 'POD_NOT_RUNNING':
            return f"Pod {issue['pod']} (namespace: {issue['namespace']}) çŠ¶æ€å¼‚å¸¸: {issue['status']}"
        
        elif issue_type == 'HIGH_CPU_USAGE':
            return f"èŠ‚ç‚¹CPUä½¿ç”¨ç‡è¿‡é«˜: {issue['usage']}"
        
        elif issue_type == 'HIGH_MEMORY_USAGE':
            return f"èŠ‚ç‚¹å†…å­˜ä½¿ç”¨ç‡è¿‡é«˜: {issue['usage']}"
        
        elif issue_type == 'PVC_NOT_BOUND':
            return f"PVC {issue['pvc']} (namespace: {issue['namespace']}) æœªç»‘å®š: {issue['status']}"
        
        elif issue_type == 'NO_NETWORK_POLICIES':
            return issue['message']
        
        return f"æœªçŸ¥é—®é¢˜: {issue}"
    
    def run_diagnostics(self):
        """è¿è¡Œæ‰€æœ‰è¯Šæ–­æ£€æŸ¥"""
        print("æ­£åœ¨è¿›è¡ŒKubernetesé›†ç¾¤è¯Šæ–­...")
        
        self.check_pod_status()
        self.check_resource_limits()
        self.check_storage_issues()
        self.check_network_policies()
        
        self.generate_report()

if __name__ == "__main__":
    troubleshooter = K8sTroubleshooter()
    troubleshooter.run_diagnostics()
```

## æ€»ç»“

Kubernetesæ˜¯ç°ä»£äº‘åŸç”Ÿåº”ç”¨çš„æ ¸å¿ƒå¹³å°ï¼ŒæŒæ¡å…¶æ ¸å¿ƒæ¦‚å¿µå’Œæœ€ä½³å®è·µè‡³å…³é‡è¦ï¼š

### å…³é”®è¦ç‚¹ï¼š
1. **å®¹å™¨ç¼–æ’**ï¼šç†è§£Podã€Serviceã€Deploymentç­‰åŸºç¡€æ¦‚å¿µ
2. **é…ç½®ç®¡ç†**ï¼šåˆç†ä½¿ç”¨ConfigMapå’ŒSecretç®¡ç†é…ç½®
3. **å­˜å‚¨æ–¹æ¡ˆ**ï¼šæ­£ç¡®é…ç½®æŒä¹…åŒ–å­˜å‚¨
4. **å®‰å…¨ç­–ç•¥**ï¼šå®æ–½RBACã€ç½‘ç»œç­–ç•¥ç­‰å®‰å…¨æªæ–½
5. **ç›‘æ§è¿ç»´**ï¼šå»ºç«‹å®Œæ•´çš„ç›‘æ§å’Œæ—¥å¿—ç³»ç»Ÿ
6. **CI/CDé›†æˆ**ï¼šè‡ªåŠ¨åŒ–éƒ¨ç½²å’Œå‘å¸ƒæµç¨‹

### å­¦ä¹ è·¯å¾„ï¼š
1. **åŸºç¡€é˜¶æ®µ**ï¼šç†è§£å®¹å™¨å’ŒKubernetesåŸºæœ¬æ¦‚å¿µ
2. **å®è·µé˜¶æ®µ**ï¼šæ­å»ºæµ‹è¯•ç¯å¢ƒï¼Œéƒ¨ç½²ç®€å•åº”ç”¨
3. **è¿›é˜¶é˜¶æ®µ**ï¼šå­¦ä¹ é«˜çº§åŠŸèƒ½ï¼Œå¦‚HPAã€ç½‘ç»œç­–ç•¥ç­‰
4. **ç”Ÿäº§é˜¶æ®µ**ï¼šæŒæ¡å®‰å…¨ã€ç›‘æ§ã€æ•…éšœæ’æŸ¥ç­‰æŠ€èƒ½

Kubernetesç”Ÿæ€ç³»ç»Ÿåºå¤§ä¸”ä¸æ–­å‘å±•ï¼ŒæŒç»­å­¦ä¹ å’Œå®è·µæ˜¯æŒæ¡è¿™é¡¹æŠ€æœ¯çš„å…³é”®ï¼

---

*ä½ åœ¨Kuberneteså®è·µä¸­é‡åˆ°è¿‡å“ªäº›æŒ‘æˆ˜ï¼Ÿæ¬¢è¿åˆ†äº«ä½ çš„è§£å†³æ–¹æ¡ˆå’Œæœ€ä½³å®è·µï¼* 